#!/usr/bin/env python

# Implementation of the IBM Model I algorithm for word alignment.
# Generates a file with alignments on the entire dataset.

# Anshul Sharma (ansharma)

import optparse
import sys
from collections import defaultdict

# Code for runtime options (copied from the default code provided)
optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]

# Initialize probabilities uniformly
t_ef = defaultdict(float)
ewords = set()
fwords = set()

for (f, e) in bitext:
	for eword in e:
		eword = eword.lower()
		ewords.add(eword)
		for fword in f:
			fword = fword.lower()
			fwords.add(fword)
			t_ef[(eword, fword)] += 1

for key in t_ef:
	t_ef[key] = 1.0/t_ef[key] 


stotal_e = defaultdict(float)

# Run the EM algorithm for five iterations
for i in range(5):
	count_ef = defaultdict(float) 
	total_f = defaultdict(float)

	# Expectation step
	for (f, e) in bitext:
		
		# Convert all words to lower case
		f = map(lambda x: x.lower(), f)
		e = map(lambda x: x.lower(), e)
		
		for eword in e:
			stotal_e[eword] = 0.0
			for fword in f:
				stotal_e[eword] += t_ef[(eword, fword)]
		for eword in e:
			for fword in f:
				count_ef[(eword, fword)] += (t_ef[(eword, fword)])/(stotal_e[eword])
				total_f[fword] += (t_ef[(eword, fword)])/(stotal_e[eword])
	
	# Maximization step
	for fword in fwords:
		for eword in ewords:
			t_ef[(eword, fword)] = count_ef[(eword, fword)]/total_f[fword]

# Creates output file named "assignment.txt" with alignments for the entire dataset
fl = open("assignment1.txt", "w")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:sys.maxint]]

for (f, e) in bitext:
        for (i, f_i) in enumerate(f):
               
		# Variables to track the maximum probability of alignment
		mx = 0.0 
                best = ()
	
                for (j, e_j) in enumerate(e):
                        if t_ef[(e_j.lower(), f_i.lower())] >= mx:
                                mx = t_ef[(e_j.lower(), f_i.lower())]
                                best = (i, j)
                fl.write("%i-%i " % best)
        fl.write("\n")
